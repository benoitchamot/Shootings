{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore all warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore')\n",
    "\n",
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import imblearn.over_sampling as OS\n",
    "\n",
    "# Import the models from SKLearn (Model 1 through Model 6)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Import the modules to evaluate the models\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# GridSearch\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local module\n",
    "from ml_classification import model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_smote(X_train, y_train, verbose=False):\n",
    "\n",
    "\tn_pos = np.sum(y_train == 1)\n",
    "\tn_neg = np.sum(y_train == 0)\n",
    "\n",
    "\t# Create Nx as many positive samples\n",
    "\tN = 50\n",
    "\tratio = {1: n_pos*N, 0: n_neg}\n",
    "\n",
    "\t# Randomly oversample\n",
    "\tROS = OS.SMOTE(\n",
    "\t\tsampling_strategy = ratio,\n",
    "\t\trandom_state = 42\n",
    "\t)\n",
    "\n",
    "\tX_train_rs, y_train_rs = ROS.fit_resample(X_train, y_train)\n",
    "\n",
    "\tif verbose:\n",
    "\t\tprint(f\"Original Pos Class Count: {np.sum(y_train)}\")\n",
    "\t\tprint(f\"Oversample Pos Class Count: {np.sum(y_train_rs)}\")\n",
    "\n",
    "\treturn X_train_rs, y_train_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, X):\n",
    "    y_pred = model.predict(X)\n",
    "    y_proba = model.predict_proba(X)[:,1]\n",
    "    # return classification and probabilities\n",
    "    return y_pred, y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPEATED FROM MODEL PERFORMANCE NOTEBOOK\n",
    "def model_performance_metrics(y_test, y_proba, y_pred, model_name='Unnamed'):\n",
    "    \n",
    "    # Calculate accuraccy, precision, recall and F1-score\n",
    "    accuracy = accuracy_score(y_test, y_pred)*100\n",
    "    precision = precision_score(y_test, y_pred)*100\n",
    "    recall = recall_score(y_test, y_pred)*100\n",
    "    f1 = f1_score(y_test, y_pred)*100\n",
    "\n",
    "    # Get P-R curve parameters (numpy arrays)\n",
    "    p_curve, r_curve, t_curve = precision_recall_curve(y_test, y_proba)\n",
    "    f1_scores = [f1_score(y_test, (y_proba >= t)) for t in t_curve]\n",
    "\n",
    "    # ROC curve and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    auc_score = auc(fpr, tpr)*100\n",
    "\n",
    "    # ROC curve parameters\n",
    "    roc_param = {\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'thr': thresholds\n",
    "    }\n",
    "\n",
    "    # Precision-Recall curve parameters\n",
    "    pr_curve = {\n",
    "        'p_curve': p_curve,\n",
    "        'r_curve': r_curve,\n",
    "        't_curve': t_curve,\n",
    "        'f1_curve': f1_scores\n",
    "    }\n",
    "\n",
    "    # Group all metrics\n",
    "    metrics = {\n",
    "        'model': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc_score': auc_score\n",
    "    }\n",
    "\n",
    "    return metrics, pr_curve, roc_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('feat_target.bin','rb') as f:\n",
    "    features_target = pickle.load(f)\n",
    "\n",
    "X = features_target['X']\n",
    "y = features_target['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1,stratify=y, test_size=0.2)\n",
    "X_train, y_train = upsample_smote(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_cv(model_type, search_type, params, scoring, n_iter=100):\n",
    "    \n",
    "    # Select model type\n",
    "    if model_type == 'DecisionTreeClassifier':\n",
    "        model = DecisionTreeClassifier()\n",
    "    elif model_type == 'LogisticRegression':\n",
    "        model = LogisticRegression()\n",
    "\n",
    "    # Select search types\n",
    "    if search_type == 'Random':\n",
    "        grid = RandomizedSearchCV(model, params, scoring=scoring, n_iter=n_iter)\n",
    "    elif search_type == 'Grid':\n",
    "        grid = GridSearchCV(model, params, scoring=scoring)\n",
    "\n",
    "    # Train models\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    return grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0: Classification Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_leaf': 5, 'max_depth': 5}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define search parameters\n",
    "params = {\n",
    "    'max_depth': np.arange(start=1, stop=11, step=1),\n",
    "    'min_samples_leaf': [2, 5, 10, 20, 100]\n",
    "}\n",
    "\n",
    "scoring='precision'\n",
    "\n",
    "best_params = search_cv('DecisionTreeClassifier', 'Random', params, scoring, n_iter=100)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'min_samples_leaf': 4}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define search parameters\n",
    "params = {\n",
    "    'max_depth': np.arange(start=1, stop=11, step=1),\n",
    "    'min_samples_leaf': np.arange(start=1, stop=21, step=1)\n",
    "}\n",
    "\n",
    "scoring='precision'\n",
    "\n",
    "best_params = search_cv('DecisionTreeClassifier', 'Grid', params, scoring)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tuned Class. Tree</td>\n",
       "      <td>98.72549</td>\n",
       "      <td>65.116279</td>\n",
       "      <td>71.794872</td>\n",
       "      <td>68.292683</td>\n",
       "      <td>88.42694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model  accuracy  precision     recall   f1_score  auc_score\n",
       "0  Tuned Class. Tree  98.72549  65.116279  71.794872  68.292683   88.42694"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initiate and fit model (unpack best parameters)\n",
    "model_0_tuned = DecisionTreeClassifier(**best_params).fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_0, y_proba_0 = get_predictions(model_0_tuned, X_test)\n",
    "\n",
    "# Get model performance metrics\n",
    "metrics_0, pr_curve_0, roc_param_0 = model_performance_metrics(y_test, y_proba_0, y_pred_0, model_name='Tuned Class. Tree')\n",
    "display(pd.DataFrame([metrics_0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'saga', 'penalty': 'l2', 'C': 0.4}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define search parameters\n",
    "params = {\n",
    "    'C': np.linspace(.1, 2, 20),\n",
    "    'penalty': ['l1', 'l2', 'elsticnet'],\n",
    "    'solver': ['saga']\n",
    "}\n",
    "\n",
    "scoring='precision'\n",
    "\n",
    "best_params_1 = search_cv('LogisticRegression', 'Random', params, scoring, n_iter=100)\n",
    "best_params_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tuned Log. Reg.</td>\n",
       "      <td>97.990196</td>\n",
       "      <td>48.076923</td>\n",
       "      <td>64.102564</td>\n",
       "      <td>54.945055</td>\n",
       "      <td>89.020874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model   accuracy  precision     recall   f1_score  auc_score\n",
       "0  Tuned Log. Reg.  97.990196  48.076923  64.102564  54.945055  89.020874"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initiate and fit model (unpack best parameters)\n",
    "model_1_tuned = LogisticRegression(**best_params_1).fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_1, y_proba_1 = get_predictions(model_1_tuned, X_test)\n",
    "\n",
    "# Get model performance metrics\n",
    "metrics_1, pr_curve_1, roc_param_1 = model_performance_metrics(y_test, y_proba_1, y_pred_1, model_name='Tuned Log. Reg.')\n",
    "display(pd.DataFrame([metrics_1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tuned Class. Tree</td>\n",
       "      <td>98.725490</td>\n",
       "      <td>65.116279</td>\n",
       "      <td>71.794872</td>\n",
       "      <td>68.292683</td>\n",
       "      <td>88.426940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuned Log. Reg.</td>\n",
       "      <td>97.990196</td>\n",
       "      <td>48.076923</td>\n",
       "      <td>64.102564</td>\n",
       "      <td>54.945055</td>\n",
       "      <td>89.020874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model   accuracy  precision     recall   f1_score  auc_score\n",
       "0  Tuned Class. Tree  98.725490  65.116279  71.794872  68.292683  88.426940\n",
       "1    Tuned Log. Reg.  97.990196  48.076923  64.102564  54.945055  89.020874"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare_models = pd.DataFrame([metrics_0, metrics_1])\n",
    "\n",
    "display(compare_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
